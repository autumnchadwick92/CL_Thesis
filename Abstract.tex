
\indent Continual Learning (CL) is a machine learning archetype where the data and learning objectives change over time \cite{DBLP:journals/corr/abs-1907-00182}. It is based around sequential learning experiences where data and objectives grow over time without past information being lost. Continual learning has gone by many names in past machine learning research such as Incremental Learning, Lifelong Learning, Perpetual Learning, and Never Ending Learning, but most modern researches have landed on Continual Learning as the main term going forward. 

\indent The focus of this discussion will be around recent research areas which strive to solve many of the problems around past algorithms. Current research is divided in 4 main categories with many crossovers between them. The categories are as follows: Regularization, Rehearsal, Generative Rehearsal, and Architectural \cite{DBLP:journals/corr/abs-1907-00182}. The main goal of the algorithms which are a part of these categories is to solve the issue of forgetting what you have already learned or Catastrophic Forgetting \cite{MCCLOSKEY1989109}. 

\indent This paper will first focus on historical work that has been done in this field, followed by current research, before moving on to discussions around Continual Learning applications in robotics. The final section of this paper will then explore a new hybrid algorithm which focuses on solving an issue with Continual Learning's use in embedded systems surrounding the lack of memory on said systems.

