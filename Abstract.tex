
\indent Continual learning (CL) is a branch of machine learning which aims create algorithms that are able to learn from real-life data sources which are naturally changing over time without forgetting what it has already learned \cite{lesort2020continual}. This issue of Catastrophic Forgetting, has plagued machine learning problems for for years and many new algorithmic approaches have been proposed to solve this issue. In this paper, we will approach this issue from a perspective of low resource utilization such as in the case with micro controllers and embedded systems.

\indent The algorithm proposed relies on the process of weighted choices, multiple networks, and a client server connection for full training, inheriting design features from both the Regularization and Architectural sides of Continual learning.\cite{DBLP:journals/corr/abs-1907-00182} The algorithm proposed follows similar patterns as other recent continual learning algorithms with following the concept of memory retention during a sleep period while also creating and merging network outputs to solve the catastrophic forgetting problem. 



